import math
import time
import re
from datetime import datetime, date, timedelta

import requests
import pandas as pd
import streamlit as st
import plotly.express as px
import streamlit.components.v1 as components

# -------------------------
# ì„¤ì •
# -------------------------
NAVER_NEWS_URL = "https://openapi.naver.com/v1/search/news.json"
OPENAI_RESPONSES_URL = "https://api.openai.com/v1/responses"

st.set_page_config(page_title="ì–¸ì–´ì™€ ë§¤ì²´: ê¸°ì‚¬ ë¶„ì„ ë„êµ¬", layout="wide")
st.title("ğŸ“° ì–¸ì–´ì™€ ë§¤ì²´ ìˆ˜í–‰í‰ê°€: ê¸°ì‚¬ ìˆ˜ì§‘ Â· ë¶„ì„ (Naver News API)")

# -------------------------
# ìœ í‹¸
# -------------------------
def normalize_keywords(raw: str) -> list[str]:
    parts = re.split(r"[,\n;]+", raw)
    cleaned = []
    for p in parts:
        k = p.strip()
        if len(k) >= 2:
            cleaned.append(k)
    seen = set()
    out = []
    for k in cleaned:
        if k not in seen:
            out.append(k)
            seen.add(k)
    return out

def clean_html(text: str) -> str:
    return re.sub(r"<[^>]+>", "", text or "")

def safe_text(s: str) -> str:
    if s is None:
        return ""
    return (
        str(s)
        .replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
    )

def parse_pubdate_to_dt(pub_raw: str):
    try:
        return datetime.strptime(pub_raw, "%a, %d %b %Y %H:%M:%S %z")
    except Exception:
        return None

def naver_api_headers():
    try:
        cid = st.secrets["NAVER_CLIENT_ID"]
        csec = st.secrets["NAVER_CLIENT_SECRET"]
    except Exception:
        st.error("Secretsì— NAVER_CLIENT_ID / NAVER_CLIENT_SECRET ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    return {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}

def get_openai_key_and_model():
    api_key = st.secrets.get("OPENAI_API_KEY", "")
    model = st.secrets.get("OPENAI_MODEL", "gpt-4o-mini")
    return api_key, model

def dedup_articles(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    if "link" in df.columns:
        df = df.drop_duplicates(subset=["link"])
    if {"title", "pubDate"}.issubset(df.columns):
        df = df.drop_duplicates(subset=["title", "pubDate"])
    return df.reset_index(drop=True)

# -------------------------
# API ìˆ˜ì§‘
# -------------------------
@st.cache_data(ttl=900, show_spinner=False)
def fetch_news_one_keyword(keyword: str, start_d: date, end_d: date, target_n: int, per_page: int = 100) -> pd.DataFrame:
    headers = naver_api_headers()
    rows = []
    start = 1
    safety_pages = 0
    max_start = 1000  # ì•ˆì „ì¥ì¹˜

    while True:
        params = {"query": keyword, "display": per_page, "start": start, "sort": "date"}
        r = requests.get(NAVER_NEWS_URL, headers=headers, params=params, timeout=20)
        if r.status_code != 200:
            raise RuntimeError(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {r.status_code} / {r.text}")

        data = r.json()
        items = data.get("items", [])
        if not items:
            break

        for it in items:
            pub_dt = parse_pubdate_to_dt(it.get("pubDate", ""))
            if pub_dt is None:
                continue

            pub_local_date = pub_dt.astimezone().date()
            if not (start_d <= pub_local_date <= end_d):
                continue

            rows.append({
                "keyword": keyword,
                "pubDate": pub_dt.astimezone().strftime("%Y-%m-%d %H:%M"),
                "title": clean_html(it.get("title", "")),
                "description": clean_html(it.get("description", "")),
                "link": it.get("link", ""),
                "originallink": it.get("originallink", ""),
            })

        if len(rows) >= target_n:
            break

        start += per_page
        safety_pages += 1
        if start > max_start or safety_pages >= 12:
            break
        time.sleep(0.2)

    return pd.DataFrame(rows)

# -------------------------
# OpenAI ë¶„ì„ (ëŒ€ì‹œë³´ë“œ í•´ì„) - Responses API
# -------------------------
def _extract_responses_text(data: dict) -> str:
    """
    Responses API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
    """
    # ë¬¸ì„œì— ë”°ë¼ output_textê°€ ì œê³µë˜ëŠ” ê²½ìš°ê°€ ìˆìŒ
    if isinstance(data, dict) and data.get("output_text"):
        return str(data["output_text"]).strip()

    out_chunks = []
    for item in data.get("output", []) if isinstance(data, dict) else []:
        for c in item.get("content", []):
            if c.get("type") == "output_text":
                out_chunks.append(c.get("text", ""))
            # ì¼ë¶€ í˜•ì‹ì—ì„œëŠ” typeì´ textì¼ ìˆ˜ë„ ìˆì–´ ë°©ì–´
            if c.get("type") == "text":
                out_chunks.append(c.get("text", ""))

    text = "".join(out_chunks).strip()
    return text

def openai_analyze_dashboard(stats_text: str) -> str:
    api_key, model = get_openai_key_and_model()
    if not api_key:
        return "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (Secrets í™•ì¸)"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    prompt = f"""
ë„ˆëŠ” ê³ 3 â€˜ì–¸ì–´ì™€ ë§¤ì²´â€™ ìˆ˜í–‰í‰ê°€ ì¡°êµë‹¤.

ê·œì¹™(ë§¤ìš° ì¤‘ìš”):
- ì•„ë˜ <í†µê³„ ìš”ì•½>ì— ìˆëŠ” ìˆ«ì/ì‚¬ì‹¤ë§Œ ì‚¬ìš©í•œë‹¤.
- í†µê³„ì— ì—†ëŠ” ë‚´ìš©(ì¶”ì •, ì¼ë°˜ë¡ , ì™¸ë¶€ì§€ì‹)ì€ ê¸ˆì§€.
- ê° ì£¼ì¥ ë¬¸ì¥ ëì— ë°˜ë“œì‹œ (ê·¼ê±°: í†µê³„ ìš”ì•½ì˜ ì–´ë–¤ í•­ëª©ì¸ì§€) í•œ ì¤„ë¡œ í‘œê¸°í•œë‹¤.

í˜•ì‹(ë°˜ë“œì‹œ ì§€ì¼œë¼):
[1] í•µì‹¬ ê´€ì°°(3~5ê°œ) : ê° ë¬¸ì¥ì— ìˆ˜ì¹˜ 1ê°œ ì´ìƒ í¬í•¨
[2] í”„ë ˆì„ í•´ì„(2~3ê°œ) : ì±…ì„ê·€ì¸/ê°ˆë“±/ê²½ì œ/í•´ê²°/ê³µí¬/ë°ì´í„° ì¤‘ ë¬´ì—‡ì´ ë³´ì´ëŠ”ì§€ + ìˆ˜ì¹˜ ê·¼ê±°
[3] ì¶”ê°€ íƒêµ¬ ì§ˆë¬¸(3ê°œ) : ê¸°ì‚¬ ë³¸ë¬¸ í™•ì¸ì´ í•„ìš”í•œ ì§ˆë¬¸ë§Œ

<í†µê³„ ìš”ì•½>
{stats_text}
""".strip()

    payload = {
        "model": model,
        # ìµœì‹  ê¶Œì¥: Responses API input
        "input": prompt,
        "max_output_tokens": 900,
    }

    last_err = None
    for _ in range(2):  # 2íšŒ ì¬ì‹œë„
        try:
            resp = requests.post(OPENAI_RESPONSES_URL, headers=headers, json=payload, timeout=90)

            if resp.status_code != 200:
                last_err = f"HTTP {resp.status_code}: {resp.text}"
                time.sleep(1)
                continue

            data = resp.json()
            text = _extract_responses_text(data)

            if not text:
                # ì§„ë‹¨ìš© ì¼ë¶€ í•„ë“œ ë…¸ì¶œ
                return f"OpenAI ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. raw_keys={list(data.keys())}"

            if len(text) < 120:
                text += "\n\nâš ï¸ ì‘ë‹µì´ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤. (ëª¨ë¸ ê¶Œí•œ/ì¿¼í„°/í•„í„°/ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ê°€ëŠ¥)"
            return text

        except Exception as e:
            last_err = repr(e)
            time.sleep(1)

    return f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {last_err}"

# -------------------------
# ë³´ê³ ì„œ HTML ìƒì„±
# -------------------------
def build_report_html(df: pd.DataFrame, evidence: dict, start_d: date, end_d: date,
                      student_id: str, student_name: str, reflection: str,
                      dashboard_ai_summary: str) -> str:
    valid_items = [(k, v) for k, v in evidence.items() if v.get("e1") and v.get("e2")]

    trs = []
    for idx, v in valid_items:
        frames = ", ".join(v.get("frame", []))
        tr = (
            "<tr>"
            f"<td>{idx}</td>"
            f"<td>{safe_text(v.get('pubDate',''))}</td>"
            f"<td>{safe_text(v.get('keyword',''))}</td>"
            f"<td>{safe_text(v.get('title',''))}</td>"
            f"<td>{safe_text(frames)}</td>"
            f"<td>{safe_text(v.get('level',''))}</td>"
            f"<td>{safe_text(v.get('e1',''))}</td>"
            f"<td>{safe_text(v.get('e2',''))}</td>"
            f"<td><a href=\"{safe_text(v.get('link',''))}\" target=\"_blank\">link</a></td>"
            "</tr>"
        )
        trs.append(tr)

    rows_html = "\n".join(trs)

    created = datetime.now().strftime("%Y-%m-%d %H:%M")
    kws = ", ".join(sorted(set(df["keyword"].tolist()))) if not df.empty else ""
    n_articles = len(df)

    html = (
        "<!doctype html>"
        "<html><head><meta charset='utf-8'/>"
        "<title>ì–¸ì–´ì™€ ë§¤ì²´ ìˆ˜í–‰í‰ê°€ ë³´ê³ ì„œ</title>"
        "<style>"
        "body{font-family:Arial, sans-serif; line-height:1.5; padding:18px;}"
        "table{border-collapse:collapse; width:100%;}"
        "th,td{border:1px solid #ccc; padding:8px; vertical-align:top;}"
        "th{background:#f2f2f2;}"
        "h1{margin-bottom:6px;}"
        ".meta{color:#555; margin:8px 0 16px 0;}"
        ".box{border:1px solid #ddd; padding:12px; background:#fafafa; white-space:pre-wrap;}"
        ".note{margin-top:14px; color:#333;}"
        "</style>"
        "</head><body>"
        "<h1>ì–¸ì–´ì™€ ë§¤ì²´ ìˆ˜í–‰í‰ê°€ ë³´ê³ ì„œ</h1>"
        f"<div class='meta'>"
        f"<b>í•™ë²ˆ</b>: {safe_text(student_id)} &nbsp;&nbsp; <b>ì„±ëª…</b>: {safe_text(student_name)}<br/>"
        f"ìƒì„± ì‹œê°: {created}<br/>"
        f"ì…ë ¥ í‚¤ì›Œë“œ: {safe_text(kws)}<br/>"
        f"ê¸°ì‚¬ ìˆ˜ì§‘ ê¸°ê°„: {start_d} ~ {end_d}<br/>"
        f"ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {n_articles}"
        f"</div>"

        "<h2>í†µê³„ ëŒ€ì‹œë³´ë“œ í•´ì„(AI)</h2>"
        f"<div class='box'>{safe_text(dashboard_ai_summary)}</div>"

        "<h2>ê°œì¸ ìƒê°(ì†Œê°/ë¹„íŒì  ê´€ì )</h2>"
        f"<div class='box'>{safe_text(reflection)}</div>"

        "<h2 style='margin-top:18px;'>Claimâ€“Evidenceâ€“Source í‘œ</h2>"
        "<p>â€» ê° í•­ëª©ì€ í•™ìƒì´ ì…ë ¥í•œ â€˜ê·¼ê±° ë¬¸ì¥â€™ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.</p>"
        "<table>"
        "<thead><tr>"
        "<th>No</th><th>ë‚ ì§œ</th><th>í‚¤ì›Œë“œ</th><th>ê¸°ì‚¬ ì œëª©</th>"
        "<th>í”„ë ˆì„</th><th>ê·¼ê±° ìˆ˜ì¤€</th><th>ê·¼ê±° ë¬¸ì¥ 1</th><th>ê·¼ê±° ë¬¸ì¥ 2</th><th>ì¶œì²˜</th>"
        "</tr></thead>"
        "<tbody>"
        f"{rows_html}"
        "</tbody>"
        "</table>"
        "<div class='note'><b>PDF ì €ì¥:</b> ì´ HTMLì„ ì—´ê³  ë¸Œë¼ìš°ì € ì¸ì‡„(Ctrl+P) â†’ â€˜PDFë¡œ ì €ì¥â€™</div>"
        "</body></html>"
    )
    return html

# -------------------------
# ì‚¬ì´ë“œë°” ì…ë ¥
# -------------------------
with st.sidebar:
    st.header("ê²€ìƒ‰ ì¡°ê±´")

    start_d, end_d = st.date_input(
        "ê¸°ê°„ ì„¤ì •",
        value=(date.today() - timedelta(days=30), date.today()),
    )

    raw_keywords = st.text_area(
        "í‚¤ì›Œë“œ ì—¬ëŸ¬ ê°œ ì…ë ¥ (ì‰¼í‘œ/ì¤„ë°”ê¿ˆ ê°€ëŠ¥)",
        value="ì €ì¶œì‚°, ì¶œìƒë¥ , ì¸êµ¬ì ˆë²½",
        height=120,
    )

    target_total = st.number_input("ëª©í‘œ ê¸°ì‚¬ ìˆ˜ (ìµœì†Œ 50)", min_value=50, value=60, step=10)
    per_keyword_cap = st.number_input("í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ëª©í‘œ(ì•ˆì „ì¥ì¹˜)", min_value=30, value=120, step=10)

    run = st.button("ìˆ˜ì§‘ ì‹œì‘", type="primary")

# -------------------------
# ìˆ˜ì§‘ ì‹¤í–‰
# -------------------------
if run:
    keywords = normalize_keywords(raw_keywords)
    if not keywords:
        st.warning("í‚¤ì›Œë“œë¥¼ 1ê°œ ì´ìƒ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì €ì¶œì‚°, ì¶œìƒë¥ )")
        st.stop()

    st.info(f"í‚¤ì›Œë“œ {len(keywords)}ê°œ: {', '.join(keywords)}")

    per_need = math.ceil(target_total / len(keywords))
    per_need = min(per_need, int(per_keyword_cap))

    frames = []
    with st.spinner("ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘..."):
        for kw in keywords:
            try:
                df_kw = fetch_news_one_keyword(kw, start_d, end_d, per_need)
                frames.append(df_kw)
            except Exception as e:
                st.error(f"'{kw}' ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    if df.empty:
        st.error("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„/í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
        st.stop()

    df = dedup_articles(df)

    if len(df) < target_total:
        st.warning(f"í˜„ì¬ {len(df)}ê°œë§Œ ìˆ˜ì§‘ë¨ â†’ ì¶”ê°€ ìˆ˜ì§‘ ì‹œë„")
        remain = target_total - len(df)
        extra = fetch_news_one_keyword(keywords[0], start_d, end_d, remain + 30)
        df = pd.concat([df, extra], ignore_index=True)
        df = dedup_articles(df)

    st.success(f"ìµœì¢… ìˆ˜ì§‘: {len(df)}ê°œ (ëª©í‘œ {target_total})")

    # âœ… rerunë¼ë„ ìœ ì§€
    st.session_state["df"] = df
    st.session_state["start_d"] = start_d
    st.session_state["end_d"] = end_d
    st.session_state["data_ready"] = True

# -------------------------
# ë©”ì¸ í‘œì‹œ(ì„¸ì…˜ì— ë°ì´í„° ìˆìœ¼ë©´ ê³„ì† ìœ ì§€)
# -------------------------
if st.session_state.get("data_ready") and "df" in st.session_state:
    df = st.session_state["df"]
    start_d = st.session_state["start_d"]
    end_d = st.session_state["end_d"]

    tabs = st.tabs(["â‘  ê¸°ì‚¬ ëª©ë¡", "â‘¡ í†µê³„ ëŒ€ì‹œë³´ë“œ", "â‘¢ ê·¼ê±° ì…ë ¥", "â‘£ ë³´ê³ ì„œ"])

    # â‘  ê¸°ì‚¬ ëª©ë¡
    with tabs[0]:
        st.subheader("â‘  ê¸°ì‚¬ ëª©ë¡")
        st.dataframe(df[["pubDate", "keyword", "title", "link"]], use_container_width=True)
        st.download_button(
            "CSV ë‹¤ìš´ë¡œë“œ(ê¸°ì‚¬ ëª©ë¡)",
            data=df.to_csv(index=False).encode("utf-8-sig"),
            file_name="articles.csv",
            mime="text/csv",
        )

    # â‘¡ í†µê³„ + OpenAI í•´ì„
    with tabs[1]:
        st.subheader("â‘¡ í†µê³„ ëŒ€ì‹œë³´ë“œ")

        df_local = df.copy()
        df_local["date"] = df_local["pubDate"].str.slice(0, 10)

        by_date = df_local.groupby("date")["title"].count().reset_index(name="count")
        st.plotly_chart(px.line(by_date, x="date", y="count", markers=True, title="ë‚ ì§œë³„ ê¸°ì‚¬ëŸ‰"), use_container_width=True)

        by_kw = df_local.groupby("keyword")["title"].count().reset_index(name="count").sort_values("count", ascending=False)
        st.plotly_chart(px.bar(by_kw, x="keyword", y="count", title="í‚¤ì›Œë“œë³„ ê¸°ì‚¬ëŸ‰"), use_container_width=True)

        st.subheader("â‘¢ ì œëª© ê°•ì¡°ì–´ ë¹ˆë„(ê°„ë‹¨)")
        hype_words = ["ì¶©ê²©", "ë…¼ë€", "íŒŒì¥", "ê¸´ê¸‰", "í­ë¡œ", "ì¶©ëŒ", "ê²½ì•…", "ë¹„ìƒ", "ì „ê²©"]
        hype_df = pd.DataFrame({
            "word": hype_words,
            "count": [int(df_local["title"].str.contains(w).sum()) for w in hype_words]
        }).sort_values("count", ascending=False)
        st.plotly_chart(px.bar(hype_df, x="word", y="count", title="ê°•ì¡°/ì„ ì • í‘œí˜„ ë¹ˆë„(ì œëª© ê¸°ì¤€)"), use_container_width=True)

        st.divider()
        st.subheader("â‘£ (ì—…ê·¸ë ˆì´ë“œ) OpenAIë¡œ í†µê³„ í•´ì„ ìƒì„±")

        top_kw = by_kw.head(10).to_dict("records")
        peak = by_date.sort_values("count", ascending=False).head(1).to_dict("records")
        hype_top = hype_df.head(6).to_dict("records")

        stats_text = (
            f"- ê¸°ê°„: {start_d} ~ {end_d}\n"
            f"- ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(df_local)}\n"
            f"- í‚¤ì›Œë“œë³„ ê¸°ì‚¬ëŸ‰(ìƒìœ„): {top_kw}\n"
            f"- ë‚ ì§œë³„ ê¸°ì‚¬ëŸ‰(í”¼í¬): {peak}\n"
            f"- ì œëª© ê°•ì¡°ì–´ ë¹ˆë„(ìƒìœ„): {hype_top}\n"
        )

        with st.expander("AIì—ê²Œ ì „ë‹¬ë˜ëŠ” í†µê³„ ìš”ì•½(ê²€ì¦ìš©)"):
            st.code(stats_text)

        if "dashboard_ai" not in st.session_state:
            st.session_state["dashboard_ai"] = ""
        if "dashboard_ai_err" not in st.session_state:
            st.session_state["dashboard_ai_err"] = ""

        if st.button("OpenAIë¡œ í†µê³„ í•´ì„ ìƒì„±", type="primary"):
            st.session_state["dashboard_ai"] = ""
            st.session_state["dashboard_ai_err"] = ""
            with st.spinner("OpenAIê°€ í†µê³„ í•´ì„ì„ ì‘ì„± ì¤‘..."):
                try:
                    st.session_state["dashboard_ai"] = openai_analyze_dashboard(stats_text)
                except Exception as e:
                    st.session_state["dashboard_ai_err"] = f"OpenAI ë¶„ì„ ì¤‘ ì˜ˆì™¸: {repr(e)}"

        if st.session_state.get("dashboard_ai_err"):
            st.error(st.session_state["dashboard_ai_err"])

        if st.session_state.get("dashboard_ai"):
            st.text_area("AI í•´ì„ ê²°ê³¼(ì „ì²´)", value=st.session_state["dashboard_ai"], height=420)

    # â‘¢ ê·¼ê±° ì…ë ¥
    with tabs[2]:
        st.subheader("â‘¢ ê·¼ê±° ì…ë ¥")
        st.write("ê¸°ì‚¬ë³„ë¡œ **ê·¼ê±° ë¬¸ì¥ 2ê°œ** + **í”„ë ˆì„**ì„ ì…ë ¥í•˜ê³  ì €ì¥í•˜ì„¸ìš”. (ì´ê²Œ ìˆì–´ì•¼ ë³´ê³ ì„œ ìƒì„± ê°€ëŠ¥)")

        if "evidence" not in st.session_state:
            st.session_state.evidence = {}

        idx = st.number_input("ê¸°ì‚¬ ë²ˆí˜¸ ì„ íƒ(0ë¶€í„°)", min_value=0, max_value=len(df)-1, value=0, step=1)
        row = df.iloc[int(idx)]

        st.markdown(f"**ì œëª©:** {row['title']}")
        st.markdown(f"**í‚¤ì›Œë“œ:** {row.get('keyword','')}")
        st.markdown(f"**ë‚ ì§œ:** {row.get('pubDate','')}")
        st.markdown(f"**ë§í¬:** {row.get('link','')}")

        saved = st.session_state.evidence.get(int(idx), {})
        e1 = st.text_area("ê·¼ê±° ë¬¸ì¥ 1(ê¸°ì‚¬ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬)", value=saved.get("e1", ""), height=80)
        e2 = st.text_area("ê·¼ê±° ë¬¸ì¥ 2(ê¸°ì‚¬ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬)", value=saved.get("e2", ""), height=80)

        frame = st.multiselect(
            "í”„ë ˆì„(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
            ["ê°ˆë“±/ëŒ€ë¦½", "ì±…ì„ ê·€ì¸", "ê²½ì œ/ë¹„ìš©", "ë„ë•/ê°€ì¹˜", "ê³µí¬/ìœ„í—˜", "í•´ê²°/ì •ì±…", "ì¸ë¬¼ ì¤‘ì‹¬", "ë°ì´í„°/ì—°êµ¬ ì¤‘ì‹¬"],
            default=saved.get("frame", [])
        )

        levels = ["ë°ì´í„°/ë³´ê³ ì„œ ëª…ì‹œ", "ì‹¤ëª… ì „ë¬¸ê°€/ê¸°ê´€ ì¸ìš©", "ë‹¹ì‚¬ì ì¸í„°ë·°", "ìµëª… ê´€ê³„ì", "ì¶”ì •/ê°€ëŠ¥ì„± í‘œí˜„ ìœ„ì£¼"]
        level_saved = saved.get("level", levels[0])
        level_index = levels.index(level_saved) if level_saved in levels else 0
        evidence_level = st.selectbox("ê·¼ê±° ìˆ˜ì¤€", levels, index=level_index)

        if st.button("ì´ ê¸°ì‚¬ ì…ë ¥ ì €ì¥", type="primary"):
            st.session_state.evidence[int(idx)] = {
                "e1": e1.strip(),
                "e2": e2.strip(),
                "frame": frame,
                "level": evidence_level,
                "title": row.get("title", ""),
                "link": row.get("link", ""),
                "pubDate": row.get("pubDate", ""),
                "keyword": row.get("keyword", ""),
            }
            st.success("ì €ì¥ ì™„ë£Œ!")

        valid = [k for k, v in st.session_state.evidence.items() if v.get("e1") and v.get("e2")]
        st.info(f"ê·¼ê±° 2ë¬¸ì¥ ì…ë ¥ ì™„ë£Œ: {len(valid)}ê°œ ê¸°ì‚¬")

    # â‘£ ë³´ê³ ì„œ
    with tabs[3]:
        st.subheader("â‘£ ë³´ê³ ì„œ")

        if "student_id" not in st.session_state:
            st.session_state["student_id"] = ""
        if "student_name" not in st.session_state:
            st.session_state["student_name"] = ""
        if "reflection" not in st.session_state:
            st.session_state["reflection"] = ""

        col1, col2 = st.columns(2)
        with col1:
            st.session_state["student_id"] = st.text_input("í•™ë²ˆ", value=st.session_state["student_id"])
        with col2:
            st.session_state["student_name"] = st.text_input("ì„±ëª…", value=st.session_state["student_name"])

        st.session_state["reflection"] = st.text_area(
            "ê°œì¸ ìƒê°(ì†Œê°/ë¹„íŒì  ê´€ì ) â€” í†µê³„+ê·¼ê±°ë¬¸ì¥ì— ê¸°ë°˜í•´ ì‘ì„±",
            value=st.session_state["reflection"],
            height=160
        )

        ev = st.session_state.get("evidence", {})
        valid_items = [(k, v) for k, v in ev.items() if v.get("e1") and v.get("e2")]

        min_required = 3
        st.write(f"ê·¼ê±° ì…ë ¥ ì™„ë£Œ ê¸°ì‚¬ ìˆ˜: **{len(valid_items)}ê°œ** / í•„ìš”: **{min_required}ê°œ**")

        ok_evidence = len(valid_items) >= min_required
        ok_student = bool(st.session_state["student_id"].strip()) and bool(st.session_state["student_name"].strip())
        ok_reflection = bool(st.session_state["reflection"].strip())
        ok_ai = bool(st.session_state.get("dashboard_ai", "").strip())

        if not ok_student:
            st.warning("í•™ë²ˆ/ì„±ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.")
        if not ok_reflection:
            st.warning("ê°œì¸ ìƒê°(ì†Œê°)ì„ ì…ë ¥í•˜ì„¸ìš”.")
        if not ok_evidence:
            st.warning("â‘¢ ê·¼ê±° ì…ë ¥ì—ì„œ ìµœì†Œ 3ê°œ ê¸°ì‚¬ì— ê·¼ê±° ë¬¸ì¥ 2ê°œë¥¼ ì…ë ¥í•˜ê³  ì €ì¥í•˜ì„¸ìš”.")
        if not ok_ai:
            st.warning("â‘¡ í†µê³„ ëŒ€ì‹œë³´ë“œì—ì„œ â€˜OpenAI í†µê³„ í•´ì„â€™ì„ ìƒì„±í•˜ë©´ ë³´ê³ ì„œ ì™„ì„±ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤. (ì„ íƒì´ì§€ë§Œ ê¶Œì¥)")

        can_make = ok_student and ok_reflection and ok_evidence

        if can_make:
            html = build_report_html(
                df=df,
                evidence=ev,
                start_d=start_d,
                end_d=end_d,
                student_id=st.session_state["student_id"],
                student_name=st.session_state["student_name"],
                reflection=st.session_state["reflection"],
                dashboard_ai_summary=st.session_state.get("dashboard_ai", "")
            )

            st.subheader("ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°")
            components.html(html, height=520, scrolling=True)

            st.download_button(
                "HTML ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ(ì¡°ê±´ ì¶©ì¡±)",
                data=html.encode("utf-8"),
                file_name="report.html",
                mime="text/html",
            )
            st.info("PDFëŠ” report.htmlì„ ì—´ê³  ë¸Œë¼ìš°ì € ì¸ì‡„(Ctrl+P) â†’ â€˜PDFë¡œ ì €ì¥â€™ì´ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤.")
        else:
            st.info("ìœ„ ì¡°ê±´ì„ ëª¨ë‘ ì±„ìš°ë©´ â€˜ë¯¸ë¦¬ë³´ê¸°â€™ì™€ â€˜ë‹¤ìš´ë¡œë“œâ€™ê°€ í™œì„±í™”ë©ë‹ˆë‹¤.")
else:
    st.caption("ì™¼ìª½ì—ì„œ ê¸°ê°„/í‚¤ì›Œë“œ ì…ë ¥ â†’ â€˜ìˆ˜ì§‘ ì‹œì‘â€™ì„ ëˆ„ë¥´ì„¸ìš”.")
