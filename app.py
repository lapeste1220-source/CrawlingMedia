import math
import time
import re
from datetime import datetime, date, timedelta
from dateutil.parser import parse as dateparse

import requests
import pandas as pd
import streamlit as st
import plotly.express as px

NAVER_NEWS_URL = "https://openapi.naver.com/v1/search/news.json"  # ê³µì‹ ì—”ë“œí¬ì¸íŠ¸ :contentReference[oaicite:3]{index=3}

st.set_page_config(page_title="ì–¸ì–´ì™€ ë§¤ì²´: ê¸°ì‚¬ ë¶„ì„ ë„êµ¬", layout="wide")
st.title("ğŸ“° ì–¸ì–´ì™€ ë§¤ì²´ ìˆ˜í–‰í‰ê°€: ê¸°ì‚¬ ìˆ˜ì§‘ Â· ë¶„ì„ (Naver News API)")

# -------------------------
# 1) ì´ˆë³´-friendly ìœ í‹¸
# -------------------------
def normalize_keywords(raw: str) -> list[str]:
    # ì‰¼í‘œ/ì¤„ë°”ê¿ˆ/ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬
    parts = re.split(r"[,\n;]+", raw)
    cleaned = []
    for p in parts:
        k = p.strip()
        if len(k) >= 2:  # ë„ˆë¬´ ì§§ì€ ê±´ ì œì™¸
            cleaned.append(k)
    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    seen = set()
    out = []
    for k in cleaned:
        if k not in seen:
            out.append(k)
            seen.add(k)
    return out

def naver_api_headers():
    # Streamlit secretsì—ì„œ í‚¤ ì½ê¸° :contentReference[oaicite:4]{index=4}
    try:
        cid = st.secrets["NAVER_CLIENT_ID"]
        csec = st.secrets["NAVER_CLIENT_SECRET"]
    except Exception:
        st.error("secrets ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. (NAVER_CLIENT_ID / NAVER_CLIENT_SECRET)")
        st.stop()

    return {
        "X-Naver-Client-Id": cid,
        "X-Naver-Client-Secret": csec,
    }

def clean_html(text: str) -> str:
    # ë„¤ì´ë²„ ê²°ê³¼ì— <b> íƒœê·¸ê°€ ì„ì—¬ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ ì œê±°
    return re.sub(r"<[^>]+>", "", text or "")

def within_range(pub_dt: datetime, start_d: date, end_d: date) -> bool:
    return (pub_dt.date() >= start_d) and (pub_dt.date() <= end_d)

# -------------------------
# 2) ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ
# -------------------------
@st.cache_data(ttl=900, show_spinner=False)
def fetch_news_one_keyword(keyword: str, start_d: date, end_d: date, target_n: int) -> pd.DataFrame:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¡œ keywordì— ëŒ€í•œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘.
    - displayëŠ” 100ê¹Œì§€ ê°€ëŠ¥(ë¬¸ì„œ ê¸°ì¤€). ì•ˆì „í•˜ê²Œ 100 ì‚¬ìš©.
    - startëŠ” 1~1000 ë²”ìœ„ì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜.
    - ê¸°ê°„ í•„í„°ëŠ” APIê°€ ì§ì ‘ ì£¼ì§€ ì•Šìœ¼ë¯€ë¡œ pubDate íŒŒì‹± í›„ ì•±ì—ì„œ ê±¸ëŸ¬ëƒ„.
    """
    headers = naver_api_headers()
    display = 100
    max_start = 1000  # ë„¤ì´ë²„ ê²€ìƒ‰ API start ì œí•œ ë²”ìœ„ ë‚´ì—ì„œë§Œ ëŒë¦¼(ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì„œ/ê´€í–‰ìƒ) :contentReference[oaicite:5]{index=5}

    rows = []
    start = 1
    safety_pages = 0

    while True:
        params = {
            "query": keyword,
            "display": display,
            "start": start,
            "sort": "date",  # ìµœì‹ ìˆœ
        }
        r = requests.get(NAVER_NEWS_URL, headers=headers, params=params, timeout=20)
        if r.status_code != 200:
            raise RuntimeError(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {r.status_code} / {r.text}")

        data = r.json()
        items = data.get("items", [])
        if not items:
            break

        for it in items:
            pub_raw = it.get("pubDate", "")
            try:
                pub_dt = dateparse(pub_raw)
            except Exception:
                continue

            # ê¸°ê°„ í•„í„°
            if not within_range(pub_dt, start_d, end_d):
                continue

            rows.append({
                "keyword": keyword,
                "pubDate": pub_dt.strftime("%Y-%m-%d %H:%M"),
                "press": clean_html(it.get("originallink", "")),  # ì›ë¬¸ ë§í¬(ë³´ì¡°)
                "title": clean_html(it.get("title", "")),
                "description": clean_html(it.get("description", "")),
                "link": it.get("link", ""),
                "originallink": it.get("originallink", ""),
            })

        # ëª©í‘œì¹˜ ë‹¬ì„±í•˜ë©´ ì¢…ë£Œ
        if len(rows) >= target_n:
            break

        # ë‹¤ìŒ í˜ì´ì§€
        start += display
        safety_pages += 1
        if start > max_start:
            break
        if safety_pages >= 12:  # ë¬´í•œ ë£¨í”„ ë°©ì§€(ìµœëŒ€ 12í˜ì´ì§€=ìµœëŒ€ 1200 ì‹œë„ ëŠë‚Œ)
            break

        time.sleep(0.2)

    df = pd.DataFrame(rows)
    return df

def dedup_articles(df: pd.DataFrame) -> pd.DataFrame:
    # link ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    if "link" in df.columns:
        df = df.drop_duplicates(subset=["link"])
    # title+pubDateë¡œ í•œ ë²ˆ ë”
    if {"title", "pubDate"}.issubset(df.columns):
        df = df.drop_duplicates(subset=["title", "pubDate"])
    return df.reset_index(drop=True)

# -------------------------
# 3) UI: ì…ë ¥
# -------------------------
with st.sidebar:
    st.header("ê²€ìƒ‰ ì¡°ê±´")

    start_d, end_d = st.date_input(
        "ê¸°ê°„ ì„¤ì •",
        value=(date.today() - timedelta(days=30), date.today()),
    )

    raw_keywords = st.text_area(
        "í‚¤ì›Œë“œ ì—¬ëŸ¬ ê°œ ì…ë ¥ (ì‰¼í‘œ/ì¤„ë°”ê¿ˆ ê°€ëŠ¥)",
        value="ì €ì¶œì‚°, ì¶œìƒë¥ , ì¸êµ¬ì ˆë²½",
        height=120,
    )

    target_total = st.number_input("ëª©í‘œ ê¸°ì‚¬ ìˆ˜ (ìµœì†Œ 50)", min_value=50, value=60, step=10)
    per_keyword_cap = st.number_input("í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ëª©í‘œ(ì•ˆì „ì¥ì¹˜)", min_value=30, value=120, step=10)

    run = st.button("ìˆ˜ì§‘ ì‹œì‘", type="primary")

# -------------------------
# 4) ì‹¤í–‰
# -------------------------
if run:
    keywords = normalize_keywords(raw_keywords)
    if not keywords:
        st.warning("í‚¤ì›Œë“œë¥¼ 1ê°œ ì´ìƒ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì €ì¶œì‚°, ì¶œìƒë¥ )")
        st.stop()

    st.info(f"í‚¤ì›Œë“œ {len(keywords)}ê°œ: {', '.join(keywords)}")
    per_need = math.ceil(target_total / len(keywords))
    per_need = min(per_need, int(per_keyword_cap))

    frames = []
    with st.spinner("ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘..."):
        for kw in keywords:
            try:
                df_kw = fetch_news_one_keyword(kw, start_d, end_d, per_need)
            except Exception as e:
                st.error(f"'{kw}' ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                df_kw = pd.DataFrame()
            frames.append(df_kw)

    df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    if df.empty:
        st.error("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„/í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
        st.stop()

    df = dedup_articles(df)

    # ë¶€ì¡±í•˜ë©´ 1ê°œ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ìˆ˜ì§‘í•´ì„œ ì±„ìš°ê¸°(ì´ˆë³´ìš© ë‹¨ìˆœ ë³´ì •)
    if len(df) < target_total:
        st.warning(f"í˜„ì¬ {len(df)}ê°œë§Œ ìˆ˜ì§‘ë¨ â†’ ì¶”ê°€ ìˆ˜ì§‘ ì‹œë„")
        remain = target_total - len(df)
        extra = fetch_news_one_keyword(keywords[0], start_d, end_d, remain + 30)
        df = pd.concat([df, extra], ignore_index=True)
        df = dedup_articles(df)

    st.success(f"ìµœì¢… ìˆ˜ì§‘: {len(df)}ê°œ (ëª©í‘œ {target_total})")

    # -------------------------
    # 5) í™”ë©´: ê¸°ì‚¬ ëª©ë¡
    # -------------------------
    st.subheader("â‘  ê¸°ì‚¬ ëª©ë¡")
    st.dataframe(df[["pubDate", "keyword", "title", "link"]], use_container_width=True)

    st.download_button(
        "CSV ë‹¤ìš´ë¡œë“œ(ê¸°ì‚¬ ëª©ë¡)",
        data=df.to_csv(index=False).encode("utf-8-sig"),
        file_name="articles.csv",
        mime="text/csv",
    )

    # -------------------------
    # 6) í™”ë©´: ê¸°ë³¸ í†µê³„ ëŒ€ì‹œë³´ë“œ
    # -------------------------
    st.subheader("â‘¡ í†µê³„ ëŒ€ì‹œë³´ë“œ")

    # ë‚ ì§œë³„ ê¸°ì‚¬ëŸ‰
    df["date"] = df["pubDate"].str.slice(0, 10)
    by_date = df.groupby("date")["title"].count().reset_index(name="count")
    fig1 = px.line(by_date, x="date", y="count", markers=True, title="ë‚ ì§œë³„ ê¸°ì‚¬ëŸ‰")
    st.plotly_chart(fig1, use_container_width=True)

    # í‚¤ì›Œë“œë³„ ê¸°ì‚¬ëŸ‰
    by_kw = df.groupby("keyword")["title"].count().reset_index(name="count").sort_values("count", ascending=False)
    fig2 = px.bar(by_kw, x="keyword", y="count", title="í‚¤ì›Œë“œë³„ ê¸°ì‚¬ëŸ‰")
    st.plotly_chart(fig2, use_container_width=True)

    # ì œëª© ê°•ì¡°ì–´(ê°„ë‹¨ ì˜ˆì‹œ)
    st.subheader("â‘¢ ì œëª© ê°•ì¡°ì–´ ë¹ˆë„(ê°„ë‹¨)")
    hype_words = ["ì¶©ê²©", "ë…¼ë€", "íŒŒì¥", "ê¸´ê¸‰", "í­ë¡œ", "ì¶©ëŒ", "ê²½ì•…", "ë¹„ìƒ", "ì „ê²©"]
    counts = []
    for w in hype_words:
        counts.append({"word": w, "count": int(df["title"].str.contains(w).sum())})
    hype_df = pd.DataFrame(counts).sort_values("count", ascending=False)
    fig3 = px.bar(hype_df, x="word", y="count", title="ê°•ì¡°/ì„ ì • í‘œí˜„ ë¹ˆë„(ì œëª© ê¸°ì¤€)")
    st.plotly_chart(fig3, use_container_width=True)

    st.info("ë‹¤ìŒ ë‹¨ê³„: ê¸°ì‚¬ë³„ â€˜ê·¼ê±° ë¬¸ì¥ 2ê°œ + í”„ë ˆì„ ì²´í¬â€™ ì…ë ¥ í™”ë©´ê³¼, HTML ë³´ê³ ì„œ ìƒì„±(â†’PDF ì €ì¥)ì„ ë¶™ì…ë‹ˆë‹¤.")
else:
    st.caption("ì™¼ìª½ì—ì„œ ê¸°ê°„/í‚¤ì›Œë“œë¥¼ ë„£ê³  â€˜ìˆ˜ì§‘ ì‹œì‘â€™ì„ ëˆ„ë¥´ì„¸ìš”.")
